[ 2024-07-09 03:54:31,081  21 COMPONENTS - INFO - Starting Data Ingestion]
[ 2024-07-09 03:54:32,153  25 COMPONENTS - INFO - Data Ingestion completed]
[ 2024-07-09 03:54:32,154  28 COMPONENTS - INFO - Text Cleaning and Preprocessing Started]
[ 2024-07-09 03:54:32,154  29 COMPONENTS - INFO - Removing all the NaN values from the title column]
[ 2024-07-09 03:54:32,341  35 COMPONENTS - INFO - Removed all NaN Values from title column]
[ 2024-07-09 03:54:32,342  36 COMPONENTS - INFO - Applying the function to concatenate two columns to form Single Title Description]
[ 2024-07-09 03:54:32,581  40 COMPONENTS - INFO - Function applied]
[ 2024-07-09 03:54:32,581  41 COMPONENTS - INFO - Removing duplicates]
[ 2024-07-09 03:54:32,603  45 COMPONENTS - INFO - Removed duplicates from titleDesc Column]
[ 2024-07-09 03:54:32,603  46 COMPONENTS - INFO - Dropping unnecessary Columns]
[ 2024-07-09 03:54:32,614  50 COMPONENTS - INFO - Dropped unnecessary Columns. Remaining columns are : Index(['Unnamed: 0', 'title', 'average_rating', 'rating_number', 'description',
       'images', 'store', 'parent_asin', 'titleDesc'],
      dtype='object')]
[ 2024-07-09 03:54:32,614  51 COMPONENTS - INFO - Lowering the case of titleDescr column]
[ 2024-07-09 03:54:32,632  55 COMPONENTS - INFO - Lowered the case of titleDesc column]
[ 2024-07-09 03:54:32,632  56 COMPONENTS - INFO - Removing Punctuations from the titleDesc column]
[ 2024-07-09 03:54:32,782  60 COMPONENTS - INFO - Removed Punctuations from the titleDesc column]
[ 2024-07-09 03:54:32,782  61 COMPONENTS - INFO - Removing stopwords from the titleDescr column]
[ 2024-07-09 03:55:33,039  65 COMPONENTS - INFO - Removed stopwords from the titleDescr column, Example value looks like : genuine leather belt men single prong metal buckle brown 4648]
[ 2024-07-09 03:55:33,039  66 COMPONENTS - INFO - Spliting the titleDescr column into list and removing duplicate words]
[ 2024-07-09 03:55:33,246  70 COMPONENTS - INFO - Splited the titleDescr column into list and removed duplicate words]
[ 2024-07-09 03:55:33,246  71 COMPONENTS - INFO - Removing all the NaN values from the titleDescr column]
[ 2024-07-09 03:55:33,379  77 COMPONENTS - INFO - Removed all NaN Values from titleDescr column]
[ 2024-07-09 03:55:33,379  78 COMPONENTS - INFO - Text Cleaning and Preprocessing completed]
[ 2024-07-09 03:55:33,379  79 COMPONENTS - INFO - Saving the transformed df into Artifact Folder]
[ 2024-07-09 03:55:34,616  83 COMPONENTS - INFO - Saved the transformed df into Artifact Folder]
[ 2024-07-09 03:55:34,616  32 COMPONENTS - INFO - Starting the training process]
[ 2024-07-09 03:55:34,616  33 COMPONENTS - INFO - Initializing W2V model with 100 feature output]
[ 2024-07-09 03:55:34,638  448 gensim.utils - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.03>', 'datetime': '2024-07-09T03:55:34.616496', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'created'}]
[ 2024-07-09 03:55:34,638  45 COMPONENTS - INFO - Initialized W2V model]
[ 2024-07-09 03:55:34,638  46 COMPONENTS - INFO - Building Vocab Count]
[ 2024-07-09 03:55:34,638  582 gensim.models.word2vec - INFO - collecting all words and their counts]
[ 2024-07-09 03:55:34,639  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types]
[ 2024-07-09 03:55:34,656  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #10000, processed 160956 words, keeping 23009 word types]
[ 2024-07-09 03:55:34,672  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #20000, processed 317897 words, keeping 35861 word types]
[ 2024-07-09 03:55:34,691  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #30000, processed 473395 words, keeping 46120 word types]
[ 2024-07-09 03:55:34,708  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #40000, processed 630506 words, keeping 55382 word types]
[ 2024-07-09 03:55:34,727  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #50000, processed 787077 words, keeping 63837 word types]
[ 2024-07-09 03:55:34,746  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #60000, processed 945699 words, keeping 71772 word types]
[ 2024-07-09 03:55:34,765  565 gensim.models.word2vec - INFO - PROGRESS: at sentence #70000, processed 1102934 words, keeping 78867 word types]
[ 2024-07-09 03:55:34,782  588 gensim.models.word2vec - INFO - collected 85241 word types from a corpus of 1243939 raw words and 78890 sentences]
[ 2024-07-09 03:55:34,782  637 gensim.models.word2vec - INFO - Creating a fresh vocabulary]
[ 2024-07-09 03:55:34,888  448 gensim.utils - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 85241 unique words (100.00% of original 85241, drops 0)', 'datetime': '2024-07-09T03:55:34.888157', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}]
[ 2024-07-09 03:55:34,888  448 gensim.utils - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1243939 word corpus (100.00% of original 1243939, drops 0)', 'datetime': '2024-07-09T03:55:34.888290', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}]
[ 2024-07-09 03:55:35,044  745 gensim.models.word2vec - INFO - deleting the raw counts dictionary of 85241 items]
[ 2024-07-09 03:55:35,045  748 gensim.models.word2vec - INFO - sample=0.001 downsamples 38 most-common words]
[ 2024-07-09 03:55:35,045  448 gensim.utils - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1157525.6751127427 word corpus (93.1%% of prior 1243939)', 'datetime': '2024-07-09T03:55:35.045686', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}]
[ 2024-07-09 03:55:35,308  805 gensim.models.word2vec - INFO - estimated required memory for 85241 words and 100 dimensions: 110813300 bytes]
[ 2024-07-09 03:55:35,308  863 gensim.models.word2vec - INFO - resetting layer weights]
[ 2024-07-09 03:55:35,348  448 gensim.utils - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-07-09T03:55:35.348670', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'build_vocab'}]
[ 2024-07-09 03:55:35,348  51 COMPONENTS - INFO - Completed Building Vocab Count]
[ 2024-07-09 03:55:35,348  52 COMPONENTS - INFO - Starting training W2Vmodel]
[ 2024-07-09 03:55:35,348  448 gensim.utils - INFO - Word2Vec lifecycle event {'msg': 'training model with 8 workers on 85241 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=20 window=3 shrink_windows=True', 'datetime': '2024-07-09T03:55:35.348870', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'train'}]
[ 2024-07-09 03:55:36,327  1652 gensim.models.word2vec - INFO - EPOCH 0: training on 1243939 raw words (1157614 effective words) took 1.0s, 1188515 effective words/s]
[ 2024-07-09 03:55:37,333  1608 gensim.models.word2vec - INFO - EPOCH 1 - PROGRESS: at 100.00% examples, 1154926 words/s, in_qsize 0, out_qsize 1]
[ 2024-07-09 03:55:37,333  1652 gensim.models.word2vec - INFO - EPOCH 1: training on 1243939 raw words (1157133 effective words) took 1.0s, 1154682 effective words/s]
[ 2024-07-09 03:55:38,182  1652 gensim.models.word2vec - INFO - EPOCH 2: training on 1243939 raw words (1157291 effective words) took 0.8s, 1370656 effective words/s]
[ 2024-07-09 03:55:39,058  1652 gensim.models.word2vec - INFO - EPOCH 3: training on 1243939 raw words (1157799 effective words) took 0.9s, 1327873 effective words/s]
[ 2024-07-09 03:55:39,892  1652 gensim.models.word2vec - INFO - EPOCH 4: training on 1243939 raw words (1157645 effective words) took 0.8s, 1394213 effective words/s]
[ 2024-07-09 03:55:40,751  1652 gensim.models.word2vec - INFO - EPOCH 5: training on 1243939 raw words (1157628 effective words) took 0.9s, 1353737 effective words/s]
[ 2024-07-09 03:55:41,756  1608 gensim.models.word2vec - INFO - EPOCH 6 - PROGRESS: at 100.00% examples, 1156506 words/s, in_qsize 0, out_qsize 1]
[ 2024-07-09 03:55:41,756  1652 gensim.models.word2vec - INFO - EPOCH 6: training on 1243939 raw words (1157657 effective words) took 1.0s, 1156370 effective words/s]
[ 2024-07-09 03:55:42,647  1652 gensim.models.word2vec - INFO - EPOCH 7: training on 1243939 raw words (1157102 effective words) took 0.9s, 1304065 effective words/s]
[ 2024-07-09 03:55:43,528  1652 gensim.models.word2vec - INFO - EPOCH 8: training on 1243939 raw words (1157855 effective words) took 0.9s, 1321241 effective words/s]
[ 2024-07-09 03:55:44,366  1652 gensim.models.word2vec - INFO - EPOCH 9: training on 1243939 raw words (1157956 effective words) took 0.8s, 1388696 effective words/s]
[ 2024-07-09 03:55:45,232  1652 gensim.models.word2vec - INFO - EPOCH 10: training on 1243939 raw words (1157520 effective words) took 0.9s, 1341514 effective words/s]
[ 2024-07-09 03:55:46,089  1652 gensim.models.word2vec - INFO - EPOCH 11: training on 1243939 raw words (1157494 effective words) took 0.9s, 1357797 effective words/s]
[ 2024-07-09 03:55:46,942  1652 gensim.models.word2vec - INFO - EPOCH 12: training on 1243939 raw words (1157599 effective words) took 0.8s, 1362430 effective words/s]
[ 2024-07-09 03:55:47,742  1652 gensim.models.word2vec - INFO - EPOCH 13: training on 1243939 raw words (1158064 effective words) took 0.8s, 1455792 effective words/s]
[ 2024-07-09 03:55:48,547  1652 gensim.models.word2vec - INFO - EPOCH 14: training on 1243939 raw words (1157227 effective words) took 0.8s, 1445114 effective words/s]
[ 2024-07-09 03:55:49,362  1652 gensim.models.word2vec - INFO - EPOCH 15: training on 1243939 raw words (1157214 effective words) took 0.8s, 1426141 effective words/s]
[ 2024-07-09 03:55:50,227  1652 gensim.models.word2vec - INFO - EPOCH 16: training on 1243939 raw words (1157173 effective words) took 0.9s, 1343658 effective words/s]
[ 2024-07-09 03:55:51,037  1652 gensim.models.word2vec - INFO - EPOCH 17: training on 1243939 raw words (1157522 effective words) took 0.8s, 1436822 effective words/s]
[ 2024-07-09 03:55:51,916  1652 gensim.models.word2vec - INFO - EPOCH 18: training on 1243939 raw words (1157710 effective words) took 0.9s, 1323085 effective words/s]
[ 2024-07-09 03:55:52,800  1652 gensim.models.word2vec - INFO - EPOCH 19: training on 1243939 raw words (1157771 effective words) took 0.9s, 1314536 effective words/s]
[ 2024-07-09 03:55:53,703  1652 gensim.models.word2vec - INFO - EPOCH 20: training on 1243939 raw words (1157801 effective words) took 0.9s, 1287795 effective words/s]
[ 2024-07-09 03:55:54,580  1652 gensim.models.word2vec - INFO - EPOCH 21: training on 1243939 raw words (1157321 effective words) took 0.9s, 1326434 effective words/s]
[ 2024-07-09 03:55:55,457  1652 gensim.models.word2vec - INFO - EPOCH 22: training on 1243939 raw words (1157388 effective words) took 0.9s, 1325970 effective words/s]
[ 2024-07-09 03:55:56,345  1652 gensim.models.word2vec - INFO - EPOCH 23: training on 1243939 raw words (1157631 effective words) took 0.9s, 1308867 effective words/s]
[ 2024-07-09 03:55:57,270  1652 gensim.models.word2vec - INFO - EPOCH 24: training on 1243939 raw words (1157445 effective words) took 0.9s, 1278865 effective words/s]
[ 2024-07-09 03:55:58,146  1652 gensim.models.word2vec - INFO - EPOCH 25: training on 1243939 raw words (1157085 effective words) took 0.9s, 1326728 effective words/s]
[ 2024-07-09 03:55:58,946  1652 gensim.models.word2vec - INFO - EPOCH 26: training on 1243939 raw words (1157689 effective words) took 0.8s, 1453922 effective words/s]
[ 2024-07-09 03:55:59,748  1652 gensim.models.word2vec - INFO - EPOCH 27: training on 1243939 raw words (1157235 effective words) took 0.8s, 1451142 effective words/s]
[ 2024-07-09 03:56:00,557  1652 gensim.models.word2vec - INFO - EPOCH 28: training on 1243939 raw words (1157698 effective words) took 0.8s, 1437262 effective words/s]
[ 2024-07-09 03:56:01,437  1652 gensim.models.word2vec - INFO - EPOCH 29: training on 1243939 raw words (1157284 effective words) took 0.9s, 1321322 effective words/s]
[ 2024-07-09 03:56:01,437  448 gensim.utils - INFO - Word2Vec lifecycle event {'msg': 'training on 37318170 raw words (34725551 effective words) took 26.1s, 1331070 effective words/s', 'datetime': '2024-07-09T03:56:01.437564', 'gensim': '4.3.2', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 12:22:05) [Clang 14.0.6 ]', 'platform': 'macOS-14.2.1-arm64-arm-64bit', 'event': 'train'}]
[ 2024-07-09 03:56:01,437  56 COMPONENTS - INFO - Completed Training W2Vmodel]
[ 2024-07-09 03:56:01,437  57 COMPONENTS - INFO - Creating a copy of df]
[ 2024-07-09 03:56:01,475  61 COMPONENTS - INFO - Created the copy of df]
[ 2024-07-09 03:56:01,475  62 COMPONENTS - INFO - Converting each word in titleDescr column(LIST) to feature representation]
[ 2024-07-09 03:56:02,058  66 COMPONENTS - INFO - Converted each word in titleDescr column(LIST) to feature representation]
[ 2024-07-09 03:56:02,058  67 COMPONENTS - INFO - Applying mean to titleDescr Colum to form a single sentence encoding]
[ 2024-07-09 03:56:02,729  71 COMPONENTS - INFO - Applied mean and formed the sentence embeddings]
[ 2024-07-09 03:56:02,729  72 COMPONENTS - INFO - Starting training of PCA model to conver 100 dimensions to 50 dimensions]
[ 2024-07-09 03:56:02,860  79 COMPONENTS - INFO - Trained the PCA model and saved the dimensionality reduced data]
[ 2024-07-09 03:56:02,860  80 COMPONENTS - INFO - Saving W2Vmodel and PCA model into pickle file in Artifact Folder]
[ 2024-07-09 03:56:02,944  85 COMPONENTS - INFO - Saved W2Vmodel and PCA model into pickle file]
[ 2024-07-09 03:56:02,945  86 COMPONENTS - INFO - Saving PCA data into Artifacts Folder]
[ 2024-07-09 03:56:05,375  92 COMPONENTS - INFO - Saved PCA data into Artifacts Folder]
